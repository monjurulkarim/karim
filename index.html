<!DOCTYPE html>
<html lang="en">
    <head>
        <meta charset="utf-8" />
        <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no" />
        <meta name="description" content="" />
        <meta name="author" content="" />
        <title>Muhammad Monjurul Karim</title>
        <link rel="icon" type="image/x-icon" href="assets/img/favicon.ico" />
        <!-- Font Awesome icons (free version)-->
        <script src="https://use.fontawesome.com/releases/v5.15.1/js/all.js" crossorigin="anonymous"></script>
        <!-- Google fonts-->
        <link href="https://fonts.googleapis.com/css?family=Saira+Extra+Condensed:500,700" rel="stylesheet" type="text/css" />
        <link href="https://fonts.googleapis.com/css?family=Muli:400,400i,800,800i" rel="stylesheet" type="text/css" />
        <!-- Core theme CSS (includes Bootstrap)-->
        <link href="css/styles.css" rel="stylesheet" />
    </head>
    <body id="page-top">
        <!-- Navigation-->
        <nav class="navbar navbar-expand-lg navbar-dark bg-primary fixed-top" id="sideNav">
            <a class="navbar-brand js-scroll-trigger" href="#page-top">
                <span class="d-block d-lg-none">Muhammad Monjurul Karim</span>
                <span class="d-none d-lg-block"><img class="img-fluid img-profile rounded-circle mx-auto mb-2" src="assets/img/profile.jpg" alt="" /></span>
            </a>
            <button class="navbar-toggler" type="button" data-toggle="collapse" data-target="#navbarSupportedContent" aria-controls="navbarSupportedContent" aria-expanded="false" aria-label="Toggle navigation"><span class="navbar-toggler-icon"></span></button>
            <div class="collapse navbar-collapse" id="navbarSupportedContent">
                <ul class="navbar-nav">
                    <li class="nav-item"><a class="nav-link js-scroll-trigger" href="#about">About</a></li>
                    <li class="nav-item"><a class="nav-link js-scroll-trigger" href="#education">Education</a></li>
                    <li class="nav-item"><a class="nav-link js-scroll-trigger" href="#research">Research</a></li>
                    <li class="nav-item"><a class="nav-link js-scroll-trigger" href="#publications">Publications</a></li>
                    <li class="nav-item"><a class="nav-link js-scroll-trigger" href="#skills">Skills</a></li>
                    <li class="nav-item"><a class="nav-link js-scroll-trigger" href="#talk">Invited Talk</a></li>
                    <li class="nav-item"><a class="nav-link js-scroll-trigger" href="#awards">Awards</a></li>
                </ul>
            </div>
        </nav>
        <!-- Page Content-->
        <div class="container-fluid p-0">
            <!-- About-->
            <section class="resume-section" id="about">
                <div class="resume-section-content">
                    <h1 class="mb-0">
                        Muhammad Monjurul
                        <span class="text-primary">Karim</span>
                    </h1>
                    <div class="subheading mb-5">
                        1208 Computer Science · Stony Brook University · Stony Brook · NY 11794 · (609) 787-9233 ·
                        <a href="mailto:muhammadmonjur.karim@stonybrook.edu">muhammadmonjur.karim@stonybrook.edu</a>
                    </div>
                    <p class="lead mb-5">I am Monjurul, a Ph.D. student of Civil Engineering at the Stony Brook University. I am working as a Research Assistant in <a href="https://sites.google.com/stonybrook.edu/rqin/home"> Dr. Ruwen Qin</a>'s Systems Analytics Laboratory. In this lab, we collaborate between Civil Infrastructure Engineering with Computer Vision Engineering. I am interested in building computer vision based deep learning models to develop advanced transportation systems and also to develop systems to provide non-contact solutions to civil infrastructure condition assessment.</p>                    
                    <p class="lead mb-5"> Please, find my CV <a href="https://drive.google.com/file/d/1jcq1fAVDR_diUBetd99vsOKdnsZk8W2j/view?usp=sharing"> here</a>.</p>
                    
                    <div class="social-icons">
<!--                         <a class="social-icon" href="https://www.linkedin.com/in/muhammad-monjurul-karim-18726279/"><i class="fab fa-linkedin-in"></i></a>
                        <a class="social-icon" href="https://github.com/monjurulkarim/"><i class="fab fa-github"></i></a> -->
                        <a href="https://www.linkedin.com/in/muhammad-monjurul-karim-18726279/"><img src="assets\img\logo-linkin.png" height="39px" style="margin-bottom:-3px; margin-right: 10px"></a>
                        <a href="https://github.com/monjurulkarim/"><img src="assets\img\github_square.png" height="44px" style="margin-bottom:-3px; margin-right: 10px"></a>
                        <a href="https://scholar.google.com/citations?user=8p93ssQAAAAJ&hl=en&oi=ao"><img src="assets\img\logo-googlescholar.png" height="38px" style="margin-bottom:-3px; margin-right: 10px"></a>
                        <!-- <a class="social-icon" href="#"><i class="fab fa-twitter"></i></a>
                        <a class="social-icon" href="https://www.facebook.com/monjurulkarimraju/"><i class="fab fa-facebook-f"></i></a> -->
                    </div>
                </div>
            </section>
            <hr class="m-0" />

            <!-- Education-->
            <section class="resume-section" id="education">
                <div class="resume-section-content">
                    <h2 class="mb-5">Education</h2>
                    <div class="d-flex flex-column flex-md-row justify-content-between mb-5">
                        <div class="flex-grow-1">
                            <h3 class="mb-0">Stony Brook University</h3>
                            <div class="subheading mb-3">Ph.D. Student</div>
                            <div>Civil Engineering</div>
                            <!-- <p>GPA: 4.00</p> -->
                        </div>
                        <div class="flex-shrink-0"><span class="text-primary">August 2020 - Present</span></div>
                    </div>
                    <div class="d-flex flex-column flex-md-row justify-content-between mb-5">
                        <div class="flex-grow-1">
                            <h3 class="mb-0">Missouri University of Science and Technology</h3>
                            <div class="subheading mb-3">Masters of Science</div>
                            <div>Systems Engineering</div>
                            <div>Thesis: Computer vision based deep learning models for cyber physical systems. |<a href="https://scholarsmine.mst.edu/masters_theses/7955/"> Download</a></div>
                            <p>GPA: 4.00/4.00</p>
                        </div>
                        <div class="flex-shrink-0"><span class="text-primary">August 2018 - July 2020</span></div>
                    </div>
                    <div class="d-flex flex-column flex-md-row justify-content-between">
                        <div class="flex-grow-1">
                            <h3 class="mb-0">Bangladesh University of Engineering and Technology</h3>
                            <div class="subheading mb-3">Bachelor of Science</div>
                            <div>Industrial and Production Engineering</div>
                            <p>GPA: 3.58/4.00</p>
                        </div>
                        <div class="flex-shrink-0"><span class="text-primary">August 2009 - June 2014</span></div>
                    </div>
                </div>
            </section>
            <hr class="m-0" />

            <!-- Research-->
            <section class="resume-section" id="research">
                <div class="resume-section-content">
                    <h2 class="mb-5">Research Projects</h2>
                    <div class="d-flex flex-column flex-md-row justify-content-between mb-5">
                        <div class="flex-grow-1">
                            <h3 class="mb-0">Bridge Inspection Video Data Analysis for Data-driven Asset Management</h3>
                            <div class="subheading mb-3"></div>
                            <img src="assets\img\fig_1_overview.PNG" alt="Overview" width="1024" height="250">
                            <div class="subheading mb-3"></div>
                            <p><b>Abstract:</b> Inspection  of  the  transportation  infrastructure,  such  as  bridges,  is  an  important  step  towards  the  preservation  andrehabilitation of the infrastructure for extending their service lives. The advancement of mobile robotic technology hasmade it possible to rapidly collect a large amount of inspection video data. Yet, the data are mainly images of complexscenes, wherein a bridge of various structural elements mix with a cluttered background. Assisting bridge inspectors inextracting structural elements of bridges from the big complex video data, and sorting them out by classes, will prepareinspectors for the element-wise inspection to determine the condition of bridges. This paper is motivated to developan assistive intelligence model for segmenting multiclass bridge elements from inspection videos captured by an aerialinspection platform. First, with a small initial training dataset labeled by inspectors, a Mask Region-based ConvolutionalNeural Network (Mask R-CNN) pre-trained on a large public dataset was transferred to the new task of multiclass bridgeelement segmentation. Then, the temporal coherence analysis attempts to recover false negative detections by thetransferred network. Finally, a semi-supervised self-training (S3T) algorithm was developed, which leverages inspectors’domain knowledge into the intelligence model by engaging them in refining the network iteratively. Quantitative andqualitative results from evaluating the developed assistive intelligence model demonstrate that the proposed methodcan utilize a small amount of time and guidance from experienced inspectors (3.58 hours for labeling 66 images) tohelp the network achieve an excellent performance (91.8% precision, 93.6% recall, and 92.7% f1-score). Importantly,the paper illustrates an approach to leveraging the domain knowledge and experiences of bridge professionals intocomputational intelligence models to efficiently adapt them to varied bridges in the National Bridge Inventory. 
                                <a href='https://github.com/monjurulkarim/active_learning'> Project Link</a>  </p>
<!--                             <div class="subsubheading mb-0"><a href='https://github.com/monjurulkarim/active_learning'> Project Link</a> </div> -->
                            <div class="subsubheading mb-0"> <b>This resarch is supported by INSPIRE University Transportation Center <a href='http://inspire-utc.mst.edu'> (http://inspire-utc.mst.edu) </a>. </b> </div>
                            
                        </div>
                        <!-- <div class="flex-shrink-0"><span class="text-primary">March 2013 - Present</span></div> -->
                    </div>
                    <div class="d-flex flex-column flex-md-row justify-content-between mb-5">
                        <div class="flex-grow-1">
                            <h3 class="mb-0">vision sensor based deep neural networks for complex driving scene analysis in support of crash risk assessment and prevention</h3>
                            <div class="subheading mb-3"></div>
                            <img src="assets\img\project2.PNG" alt="multinet" width="1024" height="250">
                            <div class="subheading mb-3"></div>
                            <p><b>Abstract:</b> To assist human drivers and autonomous vehicles in assessing crash risks, driving scene analysis using dash cameras on vehicles and deep learning algorithms is of paramount importance. Although these technologies are increasingly available, driving scene analysis for this purpose still remains a challenge. This is mainly due to the lack of annotated large image datasets for analyzing crash risk indicators and crash likelihood, and the lack of an effective method to extract lots of required information from complex driving scenes. To fill the gap, this paper develops a scene analysis system. The Multi-Net of the system includes two multi-task neural networks that perform scene classification to provide four labels for each scene. The DeepLab v3 and YOLO v3 are combined by the system to detect and locate risky pedestrians and the nearest vehicles. All identified information can provide the situational awareness to autonomous vehicles or human drivers for identifying crash risks from the surrounding traffic. To address the scarcity of annotated image datasets for studying traffic crashes, two completely new datasets have been developed by this paper and made available to the public, which were proved to be effective in training the proposed deep neural networks. The paper further evaluates the performance of the Multi-Net and the efficiency of the developed system. Comprehensive scene analysis is further illustrated with representative examples. Results demonstrate the effectiveness of the developed system and datasets for driving scene analysis, and their supportiveness for crash risk assessment and crash prevention. <a href='https://github.com/monjurulkarim/vehicle_distance'> Project Link</a></p>
                            <div class="subsubheading mb-0"> <b>This resarch is supported by MATC University Transportation Center <a href='http://matc.unl.edu/'> (http://matc.unl.edu/) </a> </b> </div>
                        </div>
                        <!-- <div class="flex-shrink-0"><span class="text-primary">December 2011 - March 2013</span></div> -->
                    </div>

                    <div class="d-flex flex-column flex-md-row justify-content-between mb-5">
                        <div class="flex-grow-1">
                            <h3 class="mb-0">Predicting future traffic accidents with vehicle mounted camera</h3>
                            <div class="subheading mb-3"></div>
                            <img src="assets\img\project3.PNG" alt="accident" width="1024" height="250">
                            <p> <b>Abstract:</b> Recently, autonomous vehicles and those equipped with an Advanced Driver Assistance System (ADAS) are emerging. They share the road with regular ones operated by human drivers entirely. To ensure guaranteed safety for passengers and other road users, it becomes essential for autonomous vehicles and ADAS to anticipate traffic accidents from natural driving scenes. The dynamic spatial-temporal interaction of the traffic agents is complex, and visual cues for predicting a future accident are embedded deeply in dashcam video data. Therefore, early anticipation of traffic accidents remains a challenge. To this end, this project developed a dynamic spatial-temporal attention (DSTA) network for early anticipation of traffic accidents from dashcam videos. The developed DSTA-network learns to select discriminative temporal segments of a video sequence with a module named Dynamic Temporal Attention (DTA). It also learns to focus on the informative spatial regions of frames with another module named Dynamic Spatial Attention (DSA). The spatial-temporal relational features of accidents, along with scene appearance features, are learned jointly with a Gated Recurrent Unit (GRU) network. The experimental evaluation of the DSTA-network on two benchmark datasets confirms that it has exceeded the state-of-the-art performance. A thorough ablation study evaluates the contributions of individual components of the DSTA-network, revealing how the network achieves such performance. Furthermore, this paper proposes a new strategy that fuses the prediction scores from two complementary models and verifies its effectiveness in further boosting the performance of early accident anticipation.
<!--                                 <a href='https://github.com/monjurulkarim/DSTA'> Project Link</a> -->
                            </p>
                            <div class="subsubheading mb-0"><b> This resarch is supported by National Science Foundation (NSF)</b></div>
                        </div>
                        <!-- <div class="flex-shrink-0"><span class="text-primary">July 2010 - December 2011</span></div> -->
                    </div>
                    <h2 class="mb-5">Other Projects</h2>
                    <div class="d-flex flex-column flex-md-row justify-content-between">
                        <div class="flex-grow-1">
                            <h3 class="mb-0">A Driving Simulator Based Study for Evaluating Safe Development of Autonomous Truck Mounted Attenuators Vehicle </h3>
                            <div class="subheading mb-3"></div>
                            <img src="assets\img\simulator.PNG" alt="simulator" width="1024" height="250">
                            <div class="subheading mb-3"></div>
                            <!-- <div class="subheading mb-3">Shout! Media Productions</div> -->
                            <p><b> Description: </b>Developed a driving simulation using blender gaming engine to collect data from drivers to better understand the impact of employing ATMA ( Autonomous Truck Mounted Attenuator) </p>
                            <div class="subsubheading mb-0"><b> This resarch is supported by <a href='https://isc.mst.edu/'>  Intelligent Systems Center at Missouri University of Science and Technology </a></b></div>
                            <div class="subheading mb-5"></div>
                        </div>
                        <!-- <div class="flex-shrink-0"><span class="text-primary">September 2008 - June 2010</span></div> -->
                    </div>


                    <div class="d-flex flex-column flex-md-row justify-content-between">
                        <div class="flex-grow-1">
                            <h3 class="mb-0"> Object detection and tracking using Mask RCNN and temporal coherence </h3>
                            <div class="subheading mb-3"></div>
                            <img src="assets\img\manufactur.gif" alt="simulator" width="512" height="250">
                            <div class="subheading mb-3"></div>
                            <!-- <div class="subheading mb-3">Shout! Media Productions</div> -->
                            <p><b> Description: </b> This is the implementation of manufacturing Object detection and tracking in the manufacturing plants. This model uses Mask RCNN model to do the initial segmentation. Which is based on Feature Pyramid Network(FPN) and a ResNet50 backbone. To give temporal consistency in the detection results, a two-staged detection threshold has been used to boost up weak detections in a frame by referring to objects with high detection scores in neighboring frames. | <a href='https://github.com/monjurulkarim/Tracking_manufacturing'> Project </a> </p>
                            <div class="subsubheading mb-0"><b> This resarch is supported by National Science Foundation (NSF)</b></div>
                            <div class="subheading mb-5"></div>

                        </div>
                        <!-- <div class="flex-shrink-0"><span class="text-primary">September 2008 - June 2010</span></div> -->
                    </div>

                  </div>

            </section>
            <hr class="m-0" />

            <!-- Publications-->
            <section class="resume-section" id="publications">
                <div class="resume-section-content">
                    <h2 class="mb-5">Publications</h2>
                    <div class="subheading mb-3">Journal Papers</div>
                    <p> 1. <b>Karim, M.M. </b>, Qin, R., Yin, Z., & Chen, G. (2021). A semi-supervised self-training method to develop assistive intelligence for segmenting multiclass bridge elements from inspection videos. <i> Structural Health Monitoring.</i> <a href="https://drive.google.com/file/d/1hemZZRymYFOfXP1g_MJQbItz48SQhhpu/view"> Download </a> | <a href='https://github.com/monjurulkarim/active_learning'> Code </a> </p> 
                    <p> 2. <b>Karim, M.M. </b>,Li, Y., Qin, R., Yin, Z. (2021). A dynamic spatial-temporal attention network for early anticipation of traffic accidents. Under Review. <a href="https://arxiv.org/abs/2106.10197"> Download </a> 
<!--                         | <a href='https://github.com/monjurulkarim/DSTA'> Code </a> </p> -->
                    <p> 3. Li, Y., <b>Karim, M.M.</b>, Qin, R., Sun, Z., Wang, Z., Yin, Z. (2021). Crash report data analysis for creating scenario-sise, spatio-temporal attention guidance to support computer vision-based perception of fatal crash risks. <i>Accident Analysis and Prevention.</i> <a href="https://www.sciencedirect.com/science/article/pii/S0001457520317826"> Download </a>  </p> 

                    <div class="subheading mb-3">Peer-Reviewed Conference Papers</div>
                    <p> 1. <b>Karim, M.M.</b>, Li, Y., Qin, R.(2021). Towards explainable artificial intelligence (XAI) for early anticipation of traffic accidents. <i>Under Review.</i> <a href='https://arxiv.org/abs/2108.00273'> Download </a> </p>
                    <p> 2. <b>Karim, M.M.</b>, Li, Y., Qin, R., Yin, Z. (2021). A system of vision sensor based deep neural networks for complex driving scene analysis in support of crashrisk assessment and prevention. <i>The 100th Transportation Research Board(TRB) Annual Meeting.</i> <a href='https://arxiv.org/abs/2106.10319'> Download </a> | <a href='https://github.com/monjurulkarim/vehicle_distance'> Code </a> </p>
                    <p> 3. <b>Karim, M.M.</b>, Dagli, CH. (2020). Sos meta-architecture selection for infrastructure inspection system using aerial drones. <i>In Proceeding of the 15th IEEE International Symposium on System of Systems Engineering (SoSE 2020).</i> Budapest, Hungary. June 2-4, 2020. <a href="https://ieeexplore.ieee.org/abstract/document/9130538"> Download </a> </p>
                    <p> 4. <b>Karim, M.M.</b>, Dagli, CH., & Qin, R. (2019). Modeling and simulation of a robotic bridge inspection system. <i>In Proceedings of the 2019 Complex Adaptive Systems Conference (CAS’19).</i> Malvern, PA. November 13-15, 2019.<a href="https://www.sciencedirect.com/science/article/pii/S1877050920304154"> Download </a> </p>
                    <p> 5. <b>Karim, M.M.</b>, Doell, D., Lingard, R., Yin, Z., Leu, MC., & Qin, R. (2019). A region-based deep learning algorithm for detecting and tracking objects in manufacturing plants. <i>In Proceedings of the 25th International Conference on Production Research (ICPR’19).</i> Chicago, IL. August 9-14, 2019.<a href="https://www.sciencedirect.com/science/article/pii/S235197892030353X"> Download </a> | <a href='https://github.com/monjurulkarim/Tracking_manufacturing'> Code </a> </p>
                    <!-- <p class="mb-0">When forced indoors, I follow a number of sci-fi and fantasy genre movies and television shows, I am an aspiring chef, and I spend a large amount of my free time exploring the latest technology advancements in the front-end web development world.</p> -->
                    <div class="subheading mb-3">Technical Reports</div>
                    <p> 1. Qin, R.,  Chen, G., Long, S.K., Yin, Z., Louis, S. <b> Karim, M.M. </b>, Zhao, T., (2020). A training framework of robotic operation and image analysis for decision-making in bridge inspection and preservation (Technical Report INSPIRE-006). USDOT INSPIRE University Transportation Center. <a href="https://inspire-utc.mst.edu/researchprojects/wd-1/"> Website </a> </p>
                    <p> 2. Qin, R., Yin, Z., <b> Karim, M.M. </b>, Li, Y., Wang, Z. (2020). Crash prediction and avoidance by identifying and evaluating risk factors from onboard cameras (Technical Report 25-1121-0005-135-2). USDOT MATC University Transportation Center. <a href="http://matc.unl.edu/research/research_project.php?researchID=560"> Download </a> </p>
                </div>
            </section>
            <hr class="m-0" />

            <!-- Skills-->
            <section class="resume-section" id="skills">
                <div class="resume-section-content">
                    <h2 class="mb-5">Skills</h2>
                    <div class="subheading mb-0">Programming Languages</div>
                    <p> Python, Matlab, SQL, PHP, HTML, CSS</p>
                    <div class="subheading mb-0">Tools and Frameworks</div>
                    <p> Pytorch, Tensorflow, OpenCV, Scikit-learn, Pandas, Numpy
                    <div class="subheading mb-3">Knowledge Base</div>
                    <ul class="fa-ul mb-0">
                        <li>
                            <span class="fa-li"><i class="fas fa-check"></i></span>
                            Deep learning
                        </li>
                        <li>
                            <span class="fa-li"><i class="fas fa-check"></i></span>
                            Cuda and GPU programming
                        </li>
                        <li>
                            <span class="fa-li"><i class="fas fa-check"></i></span>
                            Image and Video Processing
                        </li>
                        <li>
                            <span class="fa-li"><i class="fas fa-check"></i></span>
                            Object Detection and Tracking
                        </li>
                        <li>
                            <span class="fa-li"><i class="fas fa-check"></i></span>
                            Scene understanding, future anticipation
                        </li>
                        <li>
                            <span class="fa-li"><i class="fas fa-check"></i></span>
                            Feature engineering, transfer learning, incremental learning
                        </li>
                    </ul>
                </div>
            </section>
            <hr class="m-0" />
            <!-- Invited talk-->
            <section class="resume-section" id="talk">
                <div class="resume-section-content">
                    <h2 class="mb-5">Presentations (Invited)</h2>
                    <p>September 22, 2020 | 2020 System of Systems Engineering Collaborators Information Exchange (SoSECIE) Webinar organized by MITRE | <a href='https://www.youtube.com/watch?v=XPKOloDWhwE&feature=youtu.be'>  Recording </a></p>
                    </div>
            </section>
            <hr class="m-0" />
            <!-- Awards-->
            <section class="resume-section" id="awards">
                <div class="resume-section-content">
                    <h2 class="mb-5">Awards & Certifications</h2>
                    <ul class="fa-ul mb-0">
                        <li>
                            <span class="fa-li"><i class="fas fa-trophy text-warning"></i></span>
                            2
                            <sup>nd</sup>
                            Place - INSPIRE UTC Graduate Student Poster Competetion 2020
                        </li>
                        <li>
                            <span class="fa-li"><i class="fas fa-trophy text-warning"></i></span>
                            2
                            <sup>nd</sup>
                            Place - Intelligent Systems Center Poster Competion - Missouri University of Science & Technology 2019
                        </li>
                        <li>
                            <span class="fa-li"><i class="fas fa-trophy text-warning"></i></span>
                            2
                            <sup>nd</sup>
                            Place - Best Paper Award - Complex Adaptive Systems Conference 2019
                        </li>
                    </ul>
                </div>
            </section>
            <!-- Other-->
            <section class="other-section" id="others">
                <div class="sr-only", style="width:512px;height:224px>
                    <ul class="fa-ul mb-0">
                    <script type="text/javascript" id="clustrmaps" src="//clustrmaps.com/map_v2.js?d=vd3ZRyx86nwX6pEXvK45MTWhDTJg0sx2mbg3BwhcQak&cl=ffffff&w=a",  width="512" height="224"></script>
                    </ul>
                </div>
            </section>
        </div>
        <!-- Bootstrap core JS-->
        <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.5.1/jquery.min.js"></script>
        <script src="https://cdn.jsdelivr.net/npm/bootstrap@4.5.3/dist/js/bootstrap.bundle.min.js"></script>
        <!-- Third party plugin JS-->
        <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery-easing/1.4.1/jquery.easing.min.js"></script>
        <!-- Core theme JS-->
        <script src="js/scripts.js"></script>
    </body>
</html>
