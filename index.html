<!DOCTYPE html>
<html lang="en">
    <head>
        <meta charset="utf-8" />
        <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no" />
        <meta name="description" content="" />
        <meta name="author" content="" />
        <title>Muhammad Monjurul Karim</title>
        <link rel="icon" type="image/x-icon" href="assets/img/favicon.ico" />
        <!-- Font Awesome icons (free version)-->
        <script src="https://use.fontawesome.com/releases/v5.15.1/js/all.js" crossorigin="anonymous"></script>
        <!-- Google fonts-->
        <link href="https://fonts.googleapis.com/css?family=Saira+Extra+Condensed:500,700" rel="stylesheet" type="text/css" />
        <link href="https://fonts.googleapis.com/css?family=Muli:400,400i,800,800i" rel="stylesheet" type="text/css" />
        <!-- Core theme CSS (includes Bootstrap)-->
        <link href="css/styles.css" rel="stylesheet" />
    </head>
    <body id="page-top">
        <!-- Navigation-->
        <nav class="navbar navbar-expand-lg navbar-dark bg-primary fixed-top" id="sideNav">
            <a class="navbar-brand js-scroll-trigger" href="#page-top">
                <span class="d-block d-lg-none">Muhammad Monjurul Karim</span>
                <span class="d-none d-lg-block"><img class="img-fluid img-profile rounded-circle mx-auto mb-2" src="assets/img/profile.jpg" alt="" /></span>
            </a>
            <button class="navbar-toggler" type="button" data-toggle="collapse" data-target="#navbarSupportedContent" aria-controls="navbarSupportedContent" aria-expanded="false" aria-label="Toggle navigation"><span class="navbar-toggler-icon"></span></button>
            <div class="collapse navbar-collapse" id="navbarSupportedContent">
                <ul class="navbar-nav">
                    <li class="nav-item"><a class="nav-link js-scroll-trigger" href="#about">About</a></li>
                    <li class="nav-item"><a class="nav-link js-scroll-trigger" href="#education">Education</a></li>
                    <li class="nav-item"><a class="nav-link js-scroll-trigger" href="#research">Research</a></li>
                    <li class="nav-item"><a class="nav-link js-scroll-trigger" href="#publications">Publications</a></li>
                    <li class="nav-item"><a class="nav-link js-scroll-trigger" href="#skills">Skills</a></li>
                    <li class="nav-item"><a class="nav-link js-scroll-trigger" href="#talk">Invited Talk</a></li>
                    <li class="nav-item"><a class="nav-link js-scroll-trigger" href="#awards">Awards</a></li>
                </ul>
            </div>
        </nav>
        <!-- Page Content-->
        <div class="container-fluid p-0">
            <!-- About-->
            <section class="resume-section" id="about">
                <div class="resume-section-content">
                    <h1 class="mb-0">
                        Muhammad Monjurul
                        <span class="text-primary">Karim</span>
                    </h1>
                    <div class="subheading mb-5">
                        1208 Computer Science · Stony Brook University · Stony Brook · NY 11794 · (609) 787-9233 ·
                        <a href="mailto:muhammadmonjur.karim@stonybrook.edu">muhammadmonjur.karim@stonybrook.edu</a>
                    </div>
                    <p class="lead mb-5">I am Monjurul, a Ph.D. student of Civil Engineering at the Stony Brook University. I am working as a Research Assistant in <a href="https://sites.google.com/stonybrook.edu/rqin/home"> Dr. Ruwen Qin</a>'s Systems Analytics Laboratory. In this lab, we collaborate between Civil Infrastructure Engineering with Computer Vision Engineering. I am interested in building computer vision based deep learning models to develop advanced transportation systems and also to develop systems to provide non-contact solutions to civil infrastructure condition assessment.</p>                    
                    <p class="lead mb-5"> Please, find my CV <a href="https://drive.google.com/file/d/1YXIISUlr3W8i7id2IHp8T2YY3DJ_-L0S/view?usp=sharing"> here</a>.</p>
                    
                    <div class="social-icons">
                        <a class="social-icon" href="https://www.linkedin.com/in/muhammad-monjurul-karim-18726279/"><i class="fab fa-linkedin-in"></i></a>
                        <a class="social-icon" href="https://github.com/monjurulkarim/"><i class="fab fa-github"></i></a>
                        <!-- <a class="social-icon" href="#"><i class="fab fa-twitter"></i></a>
                        <a class="social-icon" href="https://www.facebook.com/monjurulkarimraju/"><i class="fab fa-facebook-f"></i></a> -->
                    </div>
                </div>
            </section>
            <hr class="m-0" />

            <!-- Education-->
            <section class="resume-section" id="education">
                <div class="resume-section-content">
                    <h2 class="mb-5">Education</h2>
                    <div class="d-flex flex-column flex-md-row justify-content-between mb-5">
                        <div class="flex-grow-1">
                            <h3 class="mb-0">Stony Brook University</h3>
                            <div class="subheading mb-3">Ph.D. Student</div>
                            <div>Civil Engineering</div>
                            <!-- <p>GPA: 4.00</p> -->
                        </div>
                        <div class="flex-shrink-0"><span class="text-primary">August 2020 - Present</span></div>
                    </div>
                    <div class="d-flex flex-column flex-md-row justify-content-between mb-5">
                        <div class="flex-grow-1">
                            <h3 class="mb-0">Missouri University of Science and Technology</h3>
                            <div class="subheading mb-3">Masters of Science</div>
                            <div>Systems Engineering</div>
                            <div>Thesis: Computer vision based deep learning models for cyber physical systems. |<a href="https://scholarsmine.mst.edu/masters_theses/7955/"> Download</a></div>
                            <p>GPA: 4.00/4.00</p>
                        </div>
                        <div class="flex-shrink-0"><span class="text-primary">August 2018 - July 2020</span></div>
                    </div>
                    <div class="d-flex flex-column flex-md-row justify-content-between">
                        <div class="flex-grow-1">
                            <h3 class="mb-0">Bangladesh University of Engineering and Technology</h3>
                            <div class="subheading mb-3">Bachelor of Science</div>
                            <div>Industrial and Production Engineering</div>
                            <p>GPA: 3.58/4.00</p>
                        </div>
                        <div class="flex-shrink-0"><span class="text-primary">August 2009 - June 2014</span></div>
                    </div>
                </div>
            </section>
            <hr class="m-0" />

            <!-- Research-->
            <section class="resume-section" id="research">
                <div class="resume-section-content">
                    <h2 class="mb-5">Research Projects</h2>
                    <div class="d-flex flex-column flex-md-row justify-content-between mb-5">
                        <div class="flex-grow-1">
                            <h3 class="mb-0">Bridge Inspection Video Data Analysis for Data-driven Asset Management</h3>
                            <div class="subheading mb-3"></div>
                            <img src="assets\img\fig_1_overview.PNG" alt="Overview" width="1024" height="250">
                            <div class="subheading mb-3"></div>
                            <p><b>Abstract:</b> Inspection  of  the  transportation  infrastructure,  such  as  bridges,  is  an  important  step  towards  the  preservation  andrehabilitation of the infrastructure for extending their service lives. The advancement of mobile robotic technology hasmade it possible to rapidly collect a large amount of inspection video data. Yet, the data are mainly images of complexscenes, wherein a bridge of various structural elements mix with a cluttered background. Assisting bridge inspectors inextracting structural elements of bridges from the big complex video data, and sorting them out by classes, will prepareinspectors for the element-wise inspection to determine the condition of bridges. This paper is motivated to developan assistive intelligence model for segmenting multiclass bridge elements from inspection videos captured by an aerialinspection platform. First, with a small initial training dataset labeled by inspectors, a Mask Region-based ConvolutionalNeural Network (Mask R-CNN) pre-trained on a large public dataset was transferred to the new task of multiclass bridgeelement segmentation. Then, the temporal coherence analysis attempts to recover false negative detections by thetransferred network. Finally, a semi-supervised self-training (S3T) algorithm was developed, which leverages inspectors’domain knowledge into the intelligence model by engaging them in refining the network iteratively. Quantitative andqualitative results from evaluating the developed assistive intelligence model demonstrate that the proposed methodcan utilize a small amount of time and guidance from experienced inspectors (3.58 hours for labeling 66 images) tohelp the network achieve an excellent performance (91.8% precision, 93.6% recall, and 92.7% f1-score). Importantly,the paper illustrates an approach to leveraging the domain knowledge and experiences of bridge professionals intocomputational intelligence models to efficiently adapt them to varied bridges in the National Bridge Inventory </p>
                            <div class="subsubheading mb-0"> <b>This resarch is supported by INSPIRE University Transportation Center <a href='http://inspire-utc.mst.edu'> (http://inspire-utc.mst.edu) </a>. </b> </div>
                        </div>
                        <!-- <div class="flex-shrink-0"><span class="text-primary">March 2013 - Present</span></div> -->
                    </div>
                    <div class="d-flex flex-column flex-md-row justify-content-between mb-5">
                        <div class="flex-grow-1">
                            <h3 class="mb-0">vision sensor based deep neural networks for complex driving scene analysis in support of crash risk assessment and prevention</h3>
                            <div class="subheading mb-3"></div>
                            <img src="assets\img\project2.PNG" alt="multinet" width="1024" height="250">
                            <div class="subheading mb-3"></div>
                            <p><b>Abstract:</b> To assist human drivers and autonomous vehicles in assessing crash risks, driving scene analysis using dash cameras on vehicles and deep learning algorithms is of paramount importance. Although these technologies are increasingly available, driving scene analysis for this purpose still remains a challenge. This is mainly due to the lack of annotated large image datasets for analyzing crash risk indicators and crash likelihood, and the lack of an effective method to extract lots of required information from complex driving scenes. To fill the gap, this paper develops a scene analysis system. The Multi-Net of the system includes two multi-task neural networks that perform scene classification to provide four labels for each scene. The DeepLab v3 and YOLO v3 are combined by the system to detect and locate risky pedestrians and the nearest vehicles. All identified information can provide the situational awareness to autonomous vehicles or human drivers for identifying crash risks from the surrounding traffic. To address the scarcity of annotated image datasets for studying traffic crashes, two completely new datasets have been developed by this paper and made available to the public, which were proved to be effective in training the proposed deep neural networks. The paper further evaluates the performance of the Multi-Net and the efficiency of the developed system. Comprehensive scene analysis is further illustrated with representative examples. Results demonstrate the effectiveness of the developed system and datasets for driving scene analysis, and their supportiveness for crash risk assessment and crash prevention. </p>
                            <div class="subsubheading mb-0"> <b>This resarch is supported by MATC University Transportation Center <a href='http://matc.unl.edu/'> (http://matc.unl.edu/) </a> </b> </div>
                        </div>
                        <!-- <div class="flex-shrink-0"><span class="text-primary">December 2011 - March 2013</span></div> -->
                    </div>

                    <div class="d-flex flex-column flex-md-row justify-content-between mb-5">
                        <div class="flex-grow-1">
                            <h3 class="mb-0">Predicting future traffic accidents with vehicle mounted camera</h3>
                            <div class="subheading mb-3"></div>
                            <p> <b>Abstract:</b> Predicting future abnormal events such as traffic violations and accidents in natural driving scenes is essential for successful autonomous driving and advanced driver assistance systems.  Predicting future abnormal events or predicting traffic accident aims to predict accidents from dashcam videos as early as possible, which is very important to have a safe and reliable driving assistance system or a fully autonomous vehicle. Accident anticipation can significantly enhance the safety level of an autonomous vehicle or driving assistance system. For example, a successful prediction only a few seconds ahead of the accident happens can help to avoid accidents and can save lives. In this project, spatial-temporal relational learning has been used to capture the relationship between accident relevant cues. To capture the spatial relationships, graph convolutional neural network (GCNN) has been used. However, accident-relevant visual cues could be overwhelmed by objects that are not relevant to the accident. Specially the false positive detections of the objects can drastically reduce the performance of the model. Therefore, Technical contribution of this project is three-fold. Firstly, to reduce the effect of false positive detection, the graph edge weight calculation has been modified by incorporating the detection score. Secondly, I used perspective transformation to get more accurate distance between object to object while calculating the relationship of the objects. And finally, the implementation of a loss function which adaptively changes the penalty based on the accident anticipation time from the previous training epoch.</p>
                        </div>
                        <!-- <div class="flex-shrink-0"><span class="text-primary">July 2010 - December 2011</span></div> -->
                    </div>
                    <h2 class="mb-5">Other Projects</h2>
                    <div class="d-flex flex-column flex-md-row justify-content-between">
                        <div class="flex-grow-1">
                            <h3 class="mb-0">A Driving Simulator Based Study for Evaluating Safe Development of Autonomous Truck Mounted Attenuators Vehicle </h3>
                            <div class="subheading mb-3"></div>
                            <img src="assets\img\simulator.PNG" alt="simulator" width="1024" height="250">
                            <div class="subheading mb-3"></div>
                            <!-- <div class="subheading mb-3">Shout! Media Productions</div> -->
                            <p><b> Description: </b>Developed a driving simulation using blender gaming engine to collect data from drivers to better understand the impact of employing ATMA ( Autonomous Truck Mounted Attenuator) </p>
                            <div class="subsubheading mb-0"><b> This resarch is supported by <a href='https://isc.mst.edu/'>  Intelligent Systems Center at Missouri University of Science and Technology </a></b></div>
                            <div class="subheading mb-5"></div>
                        </div>
                        <!-- <div class="flex-shrink-0"><span class="text-primary">September 2008 - June 2010</span></div> -->
                    </div>


                    <div class="d-flex flex-column flex-md-row justify-content-between">
                        <div class="flex-grow-1">
                            <h3 class="mb-0"> Object detection and tracking using Mask RCNN and temporal coherence </h3>
                            <div class="subheading mb-3"></div>
                            <img src="assets\img\manufactur.gif" alt="simulator" width="512" height="250">
                            <div class="subheading mb-3"></div>
                            <!-- <div class="subheading mb-3">Shout! Media Productions</div> -->
                            <p><b> Description: </b> This is the implementation of manufacturing Object detection and tracking in the manufacturing plants. This model uses Mask RCNN model to do the initial segmentation. Which is based on Feature Pyramid Network(FPN) and a ResNet50 backbone. To give temporal consistency in the detection results, a two-staged detection threshold has been used to boost up weak detections in a frame by referring to objects with high detection scores in neighboring frames. </p>
                            <div class="subsubheading mb-0"><b> This resarch is supported by National Science Foundation (NSF)</b></div>
                            <div class="subheading mb-5"></div>

                        </div>
                        <!-- <div class="flex-shrink-0"><span class="text-primary">September 2008 - June 2010</span></div> -->
                    </div>

                  </div>

            </section>
            <hr class="m-0" />

            <!-- Publications-->
            <section class="resume-section" id="publications">
                <div class="resume-section-content">
                    <h2 class="mb-5">Publications</h2>
                    <div class="subheading mb-3">Journal Papers</div>
                    <p> 1. <b>Karim, M.M. </b>, Qin, R., Yin, Z., & Chen, G. (2020). An assistive intelligence system for detecting and segmenting multiclass objects from bridge inspection videos.<i> Structural Health Monitoring.</i> Under review. </p>
                    <p> 2. Li, Y., <b>Karim, M.M.</b>, Qin, R., Sun, Z., Wang, Z., Yin, Z. (2020). Crash report data analysis for creating scenario-sise, spatio-temporal attention guidance to support computer vision-based perception of fatal crash risks. <i>Accident Analysis and Prevention.</i> <a href="https://www.sciencedirect.com/science/article/pii/S0001457520317826"> Download </a> </p> 

                    <div class="subheading mb-3">Peer-Reviewed Conference Papers</div>
                    <p> 1. <b>Karim, M.M.</b>, Li, Y., Qin, R., Yin, Z., (2021, January). A system of vision sensor based deep neural networks for complex driving scene analysis in support of crashrisk assessment and prevention. <i>The 100th Transportation Research Board(TRB) Annual Meeting.</i></p>
                    <p> 2. <b>Karim, M.M.</b>, Dagli, CH. (2020, July). Sos meta-architecture selection for infrastructure inspection system using aerial drones. <i>In Proceeding of the 15th IEEE International Symposium on System of Systems Engineering (SoSE 2020).</i> Budapest, Hungary. June 2-4, 2020. <a href="https://ieeexplore.ieee.org/abstract/document/9130538"> Download </a> </p>
                    <p> 3. <b>Karim, M.M.</b>, Dagli, CH., & Qin, R. (2019, November). Modeling and simulation of a robotic bridge inspection system. <i>In Proceedings of the 2019 Complex Adaptive Systems Conference (CAS’19).</i> Malvern, PA. November 13-15, 2019.<a href="https://www.sciencedirect.com/science/article/pii/S1877050920304154"> Download </a> </p>
                    <p> 4. <b>Karim, M.M.</b>, Doell, D., Lingard, R., Yin, Z., Leu, MC., & Qin, R. (2019, August). A region-based deep learning algorithm for detecting and tracking objects in manufacturing plants. <i>In Proceedings of the 25th International Conference on Production Research (ICPR’19).</i> Chicago, IL. August 9-14, 2019.<a href="https://www.sciencedirect.com/science/article/pii/S235197892030353X"> Download </a> </p>
                    <!-- <p class="mb-0">When forced indoors, I follow a number of sci-fi and fantasy genre movies and television shows, I am an aspiring chef, and I spend a large amount of my free time exploring the latest technology advancements in the front-end web development world.</p> -->
                </div>
            </section>
            <hr class="m-0" />

            <!-- Skills-->
            <section class="resume-section" id="skills">
                <div class="resume-section-content">
                    <h2 class="mb-5">Skills</h2>
                    <div class="subheading mb-0">Programming Languages</div>
                    <p> Python, Matlab, SQL, PHP, HTML, CSS</p>
                    <div class="subheading mb-0">Tools and Frameworks</div>
                    <p> Pytorch, Tensorflow, OpenCV, Scikit-learn, Pandas, Numpy
                    <div class="subheading mb-3">Knowledge Base</div>
                    <ul class="fa-ul mb-0">
                        <li>
                            <span class="fa-li"><i class="fas fa-check"></i></span>
                            Deep learning
                        </li>
                        <li>
                            <span class="fa-li"><i class="fas fa-check"></i></span>
                            Cuda and GPU programming
                        </li>
                        <li>
                            <span class="fa-li"><i class="fas fa-check"></i></span>
                            Image and Video Processing
                        </li>
                        <li>
                            <span class="fa-li"><i class="fas fa-check"></i></span>
                            Object Detection and Tracking
                        </li>
                        <li>
                            <span class="fa-li"><i class="fas fa-check"></i></span>
                            Scene understanding
                        </li>
                        <li>
                            <span class="fa-li"><i class="fas fa-check"></i></span>
                            Feature engineering, transfer learning, incremental learning
                        </li>
                    </ul>
                </div>
            </section>
            <hr class="m-0" />
            <!-- Invited talk-->
            <section class="resume-section" id="talk">
                <div class="resume-section-content">
                    <h2 class="mb-5">Invited Talk</h2>
                    <p>September 22, 2020 | 2020 System of Systems Engineering Collaborators Information Exchange (SoSECIE) Webinar organized by MITRE | <a href='https://www.youtube.com/watch?v=XPKOloDWhwE&feature=youtu.be'>  Recording </a></p>
                    </div>
            </section>
            <hr class="m-0" />
            <!-- Awards-->
            <section class="resume-section" id="awards">
                <div class="resume-section-content">
                    <h2 class="mb-5">Awards & Certifications</h2>
                    <ul class="fa-ul mb-0">
                        <li>
                            <span class="fa-li"><i class="fas fa-trophy text-warning"></i></span>
                            2
                            <sup>nd</sup>
                            Place - INSPIRE UTC Graduate Student Poster Competetion 2020
                        </li>
                        <li>
                            <span class="fa-li"><i class="fas fa-trophy text-warning"></i></span>
                            2
                            <sup>nd</sup>
                            Place - Intelligent Systems Center Poster Competion - Missouri University of Science & Technology 2019
                        </li>
                        <li>
                            <span class="fa-li"><i class="fas fa-trophy text-warning"></i></span>
                            2
                            <sup>nd</sup>
                            Place - Best Paper Award - Complex Adaptive Systems Conference 2019
                        </li>
                    </ul>
                </div>
            </section>
        </div>
        <!-- Bootstrap core JS-->
        <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.5.1/jquery.min.js"></script>
        <script src="https://cdn.jsdelivr.net/npm/bootstrap@4.5.3/dist/js/bootstrap.bundle.min.js"></script>
        <!-- Third party plugin JS-->
        <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery-easing/1.4.1/jquery.easing.min.js"></script>
        <!-- Core theme JS-->
        <script src="js/scripts.js"></script>
    </body>
</html>
